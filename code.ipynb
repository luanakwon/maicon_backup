{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db51b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble inference pipeline\n",
    "\n",
    "# TODO\n",
    "# locate test video folder\n",
    "# set test frame folder that will contain extracted face frames\n",
    "# gather existing files from frame folder\n",
    "\n",
    "# if all of a certain video's frame is in the folder\n",
    "# skip that video\n",
    "# else extract frames from each video\n",
    "\n",
    "# create CustomModule.py or CustomModules folder\n",
    "\n",
    "# when frames are ready\n",
    "# each models take turn and runs inference\n",
    "# result of inference is stored in padas dataframe\n",
    "\n",
    "# ensemble( choose between hard / soft voting)\n",
    "# if save sample_submission, save \n",
    "# else save as results.pickle\n",
    "\n",
    "#TODO\n",
    "# face detection py 20\n",
    "# relocate weights for facedet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23534498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from efficientnet_pytorch) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (4.8.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (3.2.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (2023.10.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (1.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (3.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
      "Using legacy 'setup.py install' for efficientnet-pytorch, since package 'wheel' is not installed.\n",
      "Installing collected packages: efficientnet-pytorch\n",
      "    Running setup.py install for efficientnet-pytorch: started\n",
      "    Running setup.py install for efficientnet-pytorch: finished with status 'done'\n",
      "Successfully installed efficientnet-pytorch-0.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\home\\elicer\\code.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/home/elicer/code.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/home/elicer/code.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/home/elicer/code.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mface_detection\u001b[39;00m \u001b[39mimport\u001b[39;00m face_extractor\n",
      "File \u001b[1;32mc:\\home\\elicer\\face_detection.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mblazeface\u001b[39;00m \u001b[39mimport\u001b[39;00m FaceExtractor, BlazeFace, VideoReader\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39marchitectures\u001b[39;00m \u001b[39mimport\u001b[39;00m fornet,weights\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39misplutils\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n",
      "File \u001b[1;32mc:\\home\\elicer\\architectures\\fornet.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn \u001b[39mas\u001b[39;00m nn\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m externals\n\u001b[0;32m     22\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mFeature Extractor\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "# import\n",
    "!pip install efficientnet_pytorch\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from face_detection import face_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c62ea117",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = '/mnt/elice/dataset/test'\n",
    "FRAME_FOLDER = '/home/elicer/data/test/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c46a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories to save frames\n",
    "os.makedirs(FRAME_FOLDER, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b91adb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "frames_per_video = 40\n",
    "real_fake_threshold = .6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run face extractrion\n",
    "face_extractor(video_path = TEST_PATH,\n",
    "               save_path = FRAME_FOLDER, \n",
    "               frames_per_video = frames_per_video, \n",
    "               skip_exist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee48e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model inference\n",
    "!python test_model.py \\\n",
    "--net EfficientNetB4 \\\n",
    "--face scale \\\n",
    "--size 224 \\\n",
    "--testsets /home/elicer/data/test/test_df.pkl \\\n",
    "--model_path /home/elicer/weights/binclass/temporary_tag4/bestval.pth \\\n",
    "--batch 16 \\\n",
    "--result_path /home/elicer/results/kwon_result.pkl\n",
    "\n",
    "# re-declare parameters\n",
    "modelname_result_path = '/home/elicer/results/kwon_result.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model inference\n",
    "!python test_model.py \\\n",
    "--net EfficientNetAutoAttB4 \\\n",
    "--face scale \\\n",
    "--size 224 \\\n",
    "--testsets /home/elicer/data/test/test_df.pkl \\\n",
    "--model_path /home/elicer/weights/binclass/choi/bestval.pth \\\n",
    "--batch 16 \\\n",
    "--result_path /home/elicer/results/choi_result.pkl\n",
    "\n",
    "# re-declare parameters\n",
    "modelname_result_path2 = '/home/elicer/results/choi_result.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ccd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model inference\n",
    "!python test_model.py \\\n",
    "--net EfficientNetB4 \\\n",
    "--face scale \\\n",
    "--size 224 \\\n",
    "--testsets /home/elicer/data/test/test_df.pkl \\\n",
    "--model_path /home/elicer/weights/binclass/s_in/it001500.pth \\\n",
    "--batch 16 \\\n",
    "--result_path /home/elicer/results/s_in_result.pkl\n",
    "\n",
    "# re-declare parameters\n",
    "modelname_result_path3 = '/home/elicer/results/s_in_result.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results and vote\n",
    "\n",
    "# load result df 1\n",
    "df = pd.read_pickle(modelname_result_path)\n",
    "# groupby and mean\n",
    "df['path'] = df['face_path'].apply(lambda x: x.split('/')[-1].split('_')[0]+'.mp4')\n",
    "df = df.drop(columns=['face_path','label'])\n",
    "result_df_1 = df.groupby(['path']).mean().reset_index()\n",
    "\n",
    "# load result df 2\n",
    "df = pd.read_pickle(modelname_result_path2)\n",
    "# groupby and mean\n",
    "df['path'] = df['face_path'].apply(lambda x: x.split('/')[-1].split('_')[0]+'.mp4')\n",
    "df = df.drop(columns=['face_path','label'])\n",
    "result_df_2 = df.groupby(['path']).mean().reset_index()\n",
    "\n",
    "# load result df 3\n",
    "df = pd.read_pickle(modelname_result_path3)\n",
    "# groupby and mean\n",
    "df['path'] = df['face_path'].apply(lambda x: x.split('/')[-1].split('_')[0]+'.mp4')\n",
    "df = df.drop(columns=['face_path','label'])\n",
    "result_df_3 = df.groupby(['path']).mean().reset_index()\n",
    "\n",
    "results = pd.merge(pd.merge(result_df_1,result_df_2, on='path'),result_df_3, on='path')\n",
    "results['avg_score'] = results[['score_x','score_y','score']].mean(axis=1)\n",
    "\n",
    "# ensemble( choose between hard / soft voting)\n",
    "# if save sample_submission, save \n",
    "# else save as results.pickle\n",
    "\n",
    "sample_sub = pd.read_csv('/home/elicer/sample_submission.csv', usecols=['path'])\n",
    "sample_sub = sample_sub.join(results[['path','avg_score']], how='left',rsuffix='tt').drop(columns=['pathtt'])\n",
    "\n",
    "sample_sub['avg_score'] = sample_sub['avg_score'].fillna(0)\n",
    "sample_sub['label'] = sample_sub['avg_score'].apply(lambda x : 'fake' if x < real_fake_threshold else 'real')\n",
    "\n",
    "sample_sub.drop('avg_score',axis=1).to_csv('/home/elicer/sample_submission.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
